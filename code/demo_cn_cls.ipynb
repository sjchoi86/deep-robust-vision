{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChoiceNet on CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,nbloader,warnings,sys\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from sklearn.utils import shuffle\n",
    "import torchvision.transforms as transforms\n",
    "from data.cifar import CIFAR10\n",
    "from util import create_gradient_clipping,gpusession,print_n_txt,augment_img,mixup\n",
    "if __name__ == \"__main__\":\n",
    "    print (\"Python version is [%s]\"%(sys.version_info[0]))\n",
    "    print (\"TensorFlow version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cn_cls_class(object):\n",
    "    def __init__(self,_name='cn_cls',_xdim=[32,32,3],_ydim=10,_hdims=[64,64],_filterSizes=[3,3],_max_pools=[2,2],_feat_dim=128,\n",
    "                 _kmix=5,_actv=tf.nn.relu,_bn=slim.batch_norm,\n",
    "                 _rho_ref_train=0.95,_tau_inv=1e-4,_pi1_bias=0.0,_logSigmaZval=-2,\n",
    "                 _logsumexp_coef=0.1,_kl_reg_coef=0.1,_l2_reg_coef=1e-5,\n",
    "                 _momentum = 0.5,\n",
    "                 _USE_INPUT_BN=False,_USE_RESNET=False,_USE_GAP=False,_USE_KENDALL_LOSS=False,_USE_SGD=False,\n",
    "                 _USE_MIXUP=False,_mixup_val=16,\n",
    "                 _GPU_ID=0,_VERBOSE=True):\n",
    "        self.name = _name\n",
    "        self.xdim = _xdim\n",
    "        self.ydim = _ydim\n",
    "        self.hdims = _hdims\n",
    "        self.filterSizes = _filterSizes\n",
    "        self.max_pools = _max_pools\n",
    "        self.feat_dim = _feat_dim\n",
    "        self.kmix = _kmix\n",
    "        self.actv = _actv \n",
    "        self.bn   = _bn # slim.batch_norm / None\n",
    "        self.rho_ref_train = _rho_ref_train\n",
    "        self.tau_inv = _tau_inv\n",
    "        self.pi1_bias = _pi1_bias\n",
    "        self.logSigmaZval = _logSigmaZval\n",
    "        self.logsumexp_coef = _logsumexp_coef\n",
    "        self.kl_reg_coef = _kl_reg_coef\n",
    "        self.l2_reg_coef = _l2_reg_coef\n",
    "        self.momentum = _momentum\n",
    "        self.USE_INPUT_BN = _USE_INPUT_BN\n",
    "        self.USE_RESNET = _USE_RESNET\n",
    "        self.USE_GAP = _USE_GAP\n",
    "        self.USE_KENDALL_LOSS = _USE_KENDALL_LOSS\n",
    "        self.USE_SGD = _USE_SGD\n",
    "        self.USE_MIXUP = _USE_MIXUP\n",
    "        self.mixup_val = _mixup_val\n",
    "        self.GPU_ID = (int)(_GPU_ID)\n",
    "        self.VERBOSE = _VERBOSE\n",
    "        with tf.device('/device:GPU:%d'%(self.GPU_ID)):\n",
    "            # Build model\n",
    "            self.build_model()\n",
    "            # Build graph\n",
    "            self.build_graph()\n",
    "            # Check parameters\n",
    "            self.check_params()\n",
    "        \n",
    "    def build_model(self):\n",
    "        _xdim = self.xdim[0]*self.xdim[1]*self.xdim[2] # Total dimension\n",
    "        self.x = tf.placeholder(dtype=tf.float32,shape=[None,_xdim],name='x') # Input [None x xdim]\n",
    "        self.t = tf.placeholder(dtype=tf.float32,shape=[None,self.ydim],name='t') # Output [None x ydim]\n",
    "        self.kp = tf.placeholder(dtype=tf.float32,shape=[],name='kp') # Keep probability \n",
    "        self.lr = tf.placeholder(dtype=tf.float32,shape=[],name='lr') # Learning rate\n",
    "        self.is_training = tf.placeholder(dtype=tf.bool,shape=[]) # Training flag\n",
    "        self.rho_ref = tf.placeholder(dtype=tf.float32,shape=[],name='rho_ref') \n",
    "        # Initailizers\n",
    "        self.fully_init  = tf.random_normal_initializer(stddev=0.01)\n",
    "        self.bias_init   = tf.constant_initializer(0.)\n",
    "        self.bn_init     = {'beta': tf.constant_initializer(0.),\n",
    "                           'gamma': tf.random_normal_initializer(1., 0.01)}\n",
    "        self.bn_params   = {'is_training':self.is_training,'decay':0.9,'epsilon':1e-5,\n",
    "                           'param_initializers':self.bn_init,'updates_collections':None}\n",
    "\n",
    "        # Build graph\n",
    "        with tf.variable_scope(self.name,reuse=False) as scope:\n",
    "            with slim.arg_scope([slim.fully_connected],activation_fn=self.actv,\n",
    "                                weights_initializer=self.fully_init,biases_initializer=self.bias_init,\n",
    "                                normalizer_fn=self.bn,normalizer_params=self.bn_params,\n",
    "                                weights_regularizer=None):            \n",
    "                \n",
    "                # List of features\n",
    "                self.layers = []\n",
    "                self.layers.append(self.x)\n",
    "\n",
    "                # Reshape input \n",
    "                _net = tf.reshape(self.x,[-1]+self.xdim) \n",
    "                self.layers.append(_net)\n",
    "\n",
    "                # Input normalization \n",
    "                if self.USE_INPUT_BN:\n",
    "                    _net = slim.batch_norm(_net,param_initializers=self.bn_init,is_training=self.is_training,updates_collections=None)\n",
    "                \n",
    "                for hidx,hdim in enumerate(self.hdims): # For all layers\n",
    "                    fs = self.filterSizes[hidx]\n",
    "                    if self.USE_RESNET: # Use residual connection \n",
    "                        cChannelSize = _net.get_shape()[3] # Current channel size\n",
    "                        if cChannelSize == hdim:\n",
    "                            _identity = _net\n",
    "                        else: # Expand dimension if required \n",
    "                            _identity = slim.conv2d(_net,hdim,[fs,fs],padding='SAME',activation_fn=None \n",
    "                                                  , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                                                  , normalizer_fn       = self.bn\n",
    "                                                  , normalizer_params   = self.bn_params\n",
    "                                                  , scope='identity_%d'%(hidx))\n",
    "                        # First conv \n",
    "                        _net = slim.conv2d(_net,hdim,[fs,fs],padding='SAME'\n",
    "                                         , activation_fn       = None \n",
    "                                         , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                                         , normalizer_fn       = self.bn\n",
    "                                         , normalizer_params   = self.bn_params\n",
    "                                         , scope='res_a_%d'%(hidx))\n",
    "                        # Relu\n",
    "                        _net = self.actv(_net)\n",
    "                        self.layers.append(_net) # Append to layers\n",
    "                        # Second conv\n",
    "                        _net = slim.conv2d(_net,hdim,[fs,fs],padding='SAME'\n",
    "                                         , activation_fn       = None\n",
    "                                         , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                                         , normalizer_fn       = self.bn\n",
    "                                         , normalizer_params   = self.bn_params\n",
    "                                         , scope='res_b_%d'%(hidx))\n",
    "                        # Skip connection\n",
    "                        _net = _net + _identity\n",
    "                        # Relu\n",
    "                        _net = self.actv(_net)\n",
    "                        self.layers.append(_net) # Append to layers\n",
    "                    else: # Without residual connection\n",
    "                        _net = slim.conv2d(_net,hdim,[fs,fs],padding='SAME'\n",
    "                                         , activation_fn       = self.actv\n",
    "                                         , weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "                                         , normalizer_fn       = self.bn\n",
    "                                         , normalizer_params   = self.bn_params\n",
    "                                         , scope='conv_%d'%(hidx))\n",
    "                    # Max pooling (if required)\n",
    "                    max_pool = self.max_pools[hidx]\n",
    "                    if max_pool > 1:\n",
    "                        _net = slim.max_pool2d(_net,[max_pool,max_pool],scope='pool_%d'%(hidx))\n",
    "                        self.layers.append(_net) # Append to layers\n",
    "                        \n",
    "                if self.USE_GAP: # Global average pooling \n",
    "                    _net = tf.reduce_mean(_net,[1,2]) # [N x R]\n",
    "                    self.layers.append(_net) # Append to layers\n",
    "                    # Optional dense layer after GAP (this increases performance)\n",
    "                    # _net = slim.fully_connected(_net,self.feat_dim,scope='gap_fc') # [N x Q]\n",
    "                    # Feature\n",
    "                    self.feat = _net # [N x Q]\n",
    "                else:\n",
    "                    # Flatten \n",
    "                    _net = slim.flatten(_net, scope='flatten')\n",
    "                    self.layers.append(_net) # Append to layers\n",
    "                    # Dense\n",
    "                    _net = slim.fully_connected(_net,self.feat_dim,scope='fc')\n",
    "                    self.layers.append(_net) # Append to layers\n",
    "                    # Feature\n",
    "                    self.feat = _net # [N x Q]\n",
    "                \n",
    "                # Feature to K rhos\n",
    "                _rho_raw = slim.fully_connected(self.feat,self.kmix,scope='rho_raw')\n",
    "                # self.rho_temp = tf.nn.tanh(_rho_raw) # [N x K] # Regression\n",
    "                self.rho_temp = tf.nn.sigmoid(_rho_raw) # [N x K] # Classification\n",
    "                self.rho = tf.concat([self.rho_temp[:,0:1]*0.0+self.rho_ref,self.rho_temp[:,1:]]\n",
    "                                     ,axis=1) # [N x K]\n",
    "                \n",
    "                # Sampler variables\n",
    "                _Q = self.feat.get_shape().as_list()[1] # Feature dimension\n",
    "                self.Q = _Q\n",
    "                self.muW = tf.get_variable(name='muW',shape=[_Q,self.ydim],\n",
    "                                          initializer=tf.random_normal_initializer(stddev=0.1)\n",
    "                                           ,dtype=tf.float32) # [Q x D]\n",
    "                self.logSigmaW = tf.get_variable(name='logSigmaW'\n",
    "                                        ,shape=[_Q,self.ydim]\n",
    "                                        ,initializer=tf.constant_initializer(-3.0)\n",
    "                                        ,dtype=tf.float32) # [Q x D]\n",
    "                self.muZ = tf.constant(np.zeros((_Q,self.ydim))\n",
    "                                        ,name='muZ',dtype=tf.float32) # [Q x D]\n",
    "                self.logSigmaZ = tf.constant(self.logSigmaZval*np.ones((_Q,self.ydim)) # -2.0 <== Important Heuristics\n",
    "                                        ,name='logSigmaZ',dtype=tf.float32) # [Q x D]\n",
    "                \n",
    "                # Make sampler\n",
    "                _N = tf.shape(self.x)[0]\n",
    "                _muW_tile = tf.tile(self.muW[tf.newaxis,:,:]\n",
    "                                    ,multiples=[_N,1,1]) # [N x Q x D]\n",
    "                _sigmaW_tile = tf.exp(tf.tile(self.logSigmaW[tf.newaxis,:,:]\n",
    "                                              ,multiples=[_N,1,1])) # [N x Q x D]\n",
    "                _muZ_tile = tf.tile(self.muZ[tf.newaxis,:,:]\n",
    "                                    ,multiples=[_N,1,1]) # [N x Q x D]\n",
    "                _sigmaZ_tile = tf.exp(tf.tile(self.logSigmaZ[tf.newaxis,:,:]\n",
    "                                              ,multiples=[_N,1,1])) # [N x Q x D]\n",
    "                samplerList = []\n",
    "                for jIdx in range(self.kmix): # For all K mixtures\n",
    "                    _rho_j = self.rho[:,jIdx:jIdx+1] # [N x 1] \n",
    "                    _rho_tile = tf.tile(_rho_j[:,:,tf.newaxis]\n",
    "                                        ,multiples=[1,_Q,self.ydim]) # [N x Q x D]\n",
    "                    _epsW = tf.random_normal(shape=[_N,_Q,self.ydim],mean=0,stddev=1\n",
    "                                             ,dtype=tf.float32) # [N x Q x D]\n",
    "                    _W = _muW_tile + tf.sqrt(_sigmaW_tile)*_epsW # [N x Q x D]\n",
    "                    _epsZ = tf.random_normal(shape=[_N,_Q,self.ydim]\n",
    "                                             ,mean=0,stddev=1,dtype=tf.float32) # [N x Q x D]\n",
    "                    _Z = _muZ_tile + tf.sqrt(_sigmaZ_tile)*_epsZ # [N x Q x D]\n",
    "                    # Append to list\n",
    "                    _Y = _rho_tile*_muW_tile + (1.0-_rho_tile**2) \\\n",
    "                        *(_rho_tile*tf.sqrt(_sigmaZ_tile)/tf.sqrt(_sigmaW_tile) \\\n",
    "                              *(_W-_muW_tile)+tf.sqrt(1-_rho_tile**2)*_Z)\n",
    "                    samplerList.append(_Y) # Append \n",
    "                # Make list to tensor\n",
    "                WlistConcat = tf.convert_to_tensor(samplerList) # K*[N x Q x D] => [K x N x Q x D]\n",
    "                self.wSample = tf.transpose(WlistConcat,perm=[1,3,0,2]) # [N x D x K x Q]\n",
    "\n",
    "                # K mean mixtures [N x D x K]\n",
    "                _wTemp = tf.reshape(self.wSample\n",
    "                                ,shape=[_N,self.kmix*self.ydim,_Q]) # [N x KD x Q]\n",
    "                _featRsh = tf.reshape(self.feat,shape=[_N,_Q,1]) # [N x Q x 1]\n",
    "                _mu = tf.matmul(_wTemp,_featRsh) # [N x KD x Q] x [N x Q x 1] => [N x KD x 1]\n",
    "                self.mu = tf.reshape(_mu,shape=[_N,self.ydim,self.kmix]) # [N x D x K]\n",
    "                \n",
    "                # (optional) Add bias to mu\n",
    "                USE_BIAS = False\n",
    "                if USE_BIAS:\n",
    "                    self.muBias = tf.get_variable(name='muBias'\n",
    "                                            ,shape=[self.ydim]\n",
    "                                            ,initializer=tf.constant_initializer(0.0)\n",
    "                                            ,dtype=tf.float32) # [D]\n",
    "                    muBias_tile = tf.tile(self.muBias[tf.newaxis,:,tf.newaxis]\n",
    "                                        ,multiples=[_N,1,self.kmix]) # [N x D x K]\n",
    "                    self.mu += muBias_tile\n",
    "\n",
    "                # K var mixtures [N x D x K]\n",
    "                _logvar_raw = slim.fully_connected(self.feat,self.ydim,scope='var_raw') # [N x D]\n",
    "                _var_raw = tf.exp(_logvar_raw) # [N x D]\n",
    "                _var_tile = tf.tile(_var_raw[:,:,tf.newaxis]\n",
    "                                    ,multiples=[1,1,self.kmix]) # [N x D x K]\n",
    "                _rho_tile = tf.tile(self.rho[:,tf.newaxis,:]\n",
    "                                    ,multiples=[1,self.ydim,1]) # [N x D x K]\n",
    "                _tau_inv = self.tau_inv\n",
    "                self.var = (1.0-_rho_tile**2)*_var_tile + _tau_inv # [N x D x K]\n",
    "                \n",
    "                # Weight allocation probability pi [N x K]\n",
    "                _pi_logits = slim.fully_connected(self.feat,self.kmix\n",
    "                                                  ,scope='pi_logits') # [N x K]\n",
    "                self.pi_temp = tf.nn.softmax(_pi_logits,dim=1) # [N x K]\n",
    "                # Some heuristics to ensure that pi_1(x) is high enough\n",
    "                self.pi_temp = tf.concat([self.pi_temp[:,0:1]+self.pi1_bias\n",
    "                                          ,self.pi_temp[:,1:]],axis=1) # [N x K]\n",
    "                self.pi = tf.nn.softmax(self.pi_temp,dim=1) # [N x K]\n",
    "                \n",
    "                # Intermediate tensors\n",
    "                self.tensors = [self.x,self.feat,self.rho,self.mu,self.var,self.pi] \n",
    "    \n",
    "    # Build graph\n",
    "    def build_graph(self):\n",
    "        # MDN loss\n",
    "        _N = tf.shape(self.x)[0]\n",
    "        t,mu,var = self.t,self.mu,self.var\n",
    "        pi = self.pi # [N x K]\n",
    "        yhat = mu + tf.sqrt(var)*tf.random_normal(shape=[_N,self.ydim,self.kmix]) # Sampled y [N x D x K]\n",
    "        tTile = tf.tile(t[:,:,tf.newaxis],[1,1,self.kmix]) # Target [N x D x K]\n",
    "        piTile = tf.tile(pi[:,tf.newaxis,:],[1,self.ydim,1]) # piTile: [N x D x K]\n",
    "        \n",
    "        if self.USE_KENDALL_LOSS: # Alex Kendal's loss extended to a mixture model\n",
    "            self._loss_fit = tf.reduce_sum(-piTile*yhat*tTile,axis=[1,2]) # [N]\n",
    "            self.loss_fit = tf.reduce_mean(self._loss_fit) # [1]\n",
    "            \n",
    "            self._loss_reg = pi*tf.reduce_logsumexp(yhat,axis=[1]) # [N x K]\n",
    "            self.__loss_reg = tf.reduce_sum(self._loss_reg,axis=[1]) # [N]\n",
    "            self.loss_reg = tf.reduce_mean(self.__loss_reg) # [1] \n",
    "            \n",
    "            # self._loss_reg = tf.reduce_logsumexp(piTile*yhat,axis=[1,2]) # [N]\n",
    "            # self.loss_reg = tf.reduce_mean(self._loss_reg) # [1]\n",
    "        else: # Mine (normalized x)\n",
    "            self.yhat_normalized = tf.nn.softmax(yhat,dim=1) # [N x D x K]\n",
    "            self._loss_fit = tf.reduce_sum(-piTile*self.yhat_normalized*tTile,axis=[1,2]) # [N]\n",
    "            self.loss_fit = tf.reduce_mean(self._loss_fit) # [1]\n",
    "            \n",
    "            self._loss_reg = pi*tf.reduce_logsumexp(yhat,axis=[1]) # [N x K]\n",
    "            self.__loss_reg = tf.reduce_sum(self._loss_reg,axis=[1]) # [N]\n",
    "            self.loss_reg = self.logsumexp_coef*tf.reduce_mean(self.__loss_reg) # [1] \n",
    "            \n",
    "            # self._loss_reg = self.logsumexp_coef*tf.reduce_logsumexp(piTile*yhat,axis=[1,2]) # [N]\n",
    "            # self.loss_reg = tf.reduce_mean(self._loss_reg) # [1]\n",
    "        \n",
    "        # KL-divergence regularizer \n",
    "        _eps = 1e-8\n",
    "        self._kl_reg = self.kl_reg_coef*tf.reduce_sum(-self.rho\n",
    "                        *(tf.log(self.pi+_eps)-tf.log(self.rho+_eps)),axis=1) # (N)\n",
    "        self.kl_reg = tf.reduce_mean(self._kl_reg) # (1)\n",
    "        \n",
    "        # Weight decay \n",
    "        # _g_vars = tf.global_variables()\n",
    "        _g_vars = tf.trainable_variables()\n",
    "        self.c_vars = [var for var in _g_vars if '%s/'%(self.name) in var.name]\n",
    "        self.l2_reg = self.l2_reg_coef*tf.reduce_sum(tf.stack([tf.nn.l2_loss(v) for v in self.c_vars])) # [1]\n",
    "\n",
    "        # Total loss\n",
    "        self.loss_total = tf.reduce_mean(self.loss_fit+self.loss_reg+self.kl_reg+self.l2_reg) # [1]\n",
    "        # Optimizer\n",
    "        GRAD_CLIP = True\n",
    "        if GRAD_CLIP: # Gradient clipping\n",
    "            if self.USE_SGD:\n",
    "                # _optm = tf.train.GradientDescentOptimizer(learning_rate=self.lr)\n",
    "                _optm = tf.train.MomentumOptimizer(learning_rate=self.lr,momentum=self.momentum)\n",
    "            else:\n",
    "                _optm = tf.train.AdamOptimizer(learning_rate=self.lr\n",
    "                                               ,beta1=0.9,beta2=0.999,epsilon=1e-6)\n",
    "            self.optm = create_gradient_clipping(self.loss_total\n",
    "                                            ,_optm,tf.trainable_variables(),clipVal=1.0)\n",
    "        else:\n",
    "            if self.USE_SGD:\n",
    "                self.optm = tf.train.GradientDescentOptimizer(learning_rate=self.lr).minimize(self.loss_total) \n",
    "            else:\n",
    "                self.optm = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss_total) \n",
    "        \n",
    "        # Compute accuray \n",
    "        maxIdx = tf.argmax(input=pi,axis=1, output_type=tf.int32) # Argmax Index [N]\n",
    "        maxIdx = 0*tf.ones_like(maxIdx)\n",
    "        coords = tf.stack([tf.transpose(gv) for gv in tf.meshgrid(tf.range(_N),tf.range(self.ydim))] + \n",
    "                          [tf.reshape(tf.tile(maxIdx[:,tf.newaxis],[1,self.ydim]),shape=(_N,self.ydim))]\n",
    "                          ,axis=2) # [N x D x 3]\n",
    "        mu_bar = tf.gather_nd(mu,coords) # [N x D]\n",
    "        _corr = tf.equal(tf.argmax(mu_bar, 1), tf.argmax(self.t, 1))    \n",
    "        self.accr = tf.reduce_mean(tf.cast(_corr,tf.float32)) # Accuracy\n",
    "        \n",
    "    # Check parameters\n",
    "    def check_params(self):\n",
    "        _g_vars = tf.global_variables()\n",
    "        self.g_vars = [var for var in _g_vars if '%s/'%(self.name) in var.name]\n",
    "        if self.VERBOSE:\n",
    "            print (\"==== Global Variables ====\")\n",
    "        for i in range(len(self.g_vars)):\n",
    "            w_name  = self.g_vars[i].name\n",
    "            w_shape = self.g_vars[i].get_shape().as_list()\n",
    "            if self.VERBOSE:\n",
    "                print (\" [%02d] Name:[%s] Shape:[%s]\" % (i,w_name,w_shape))\n",
    "        # Print layers\n",
    "        if self.VERBOSE:\n",
    "            print (\"Layers:\")\n",
    "            nLayers = len(self.layers)\n",
    "            for i in range(nLayers):\n",
    "                print (\"[%02d/%d] %s %s\"%(i,nLayers,self.layers[i].name,self.layers[i].shape))\n",
    "    \n",
    "    # Sampler\n",
    "    def sampler(self,_sess,_x,n_samples=10):\n",
    "        pi, mu, var = _sess.run([self.pi, self.mu, self.var],\n",
    "                                feed_dict={self.x:_x,self.kp:1.0,self.is_training:False\n",
    "                                          ,self.rho_ref:1.0}) #\n",
    "        n_points = _x.shape[0]\n",
    "        _y_sampled = np.zeros([n_points,self.ydim,n_samples])\n",
    "        for i in range(n_points):\n",
    "            for j in range(n_samples):\n",
    "                k = np.random.choice(self.kmix,p=pi[i,:])\n",
    "                k = 0\n",
    "                _y_sampled[i,:,j] = mu[i,:,k] # + np.random.randn(1,self.ydim)*np.sqrt(var[i,:,k])\n",
    "        return _y_sampled\n",
    "    \n",
    "    # Save \n",
    "    def save(self,_sess,_savename=None):\n",
    "        \"\"\" Save name \"\"\"\n",
    "        if _savename==None:\n",
    "            _savename='net/net_%s.npz'%(self.name)\n",
    "        \"\"\" Get global variables \"\"\"\n",
    "        self.g_wnames,self.g_wvals,self.g_wshapes = [],[],[]\n",
    "        for i in range(len(self.g_vars)):\n",
    "            curr_wname = self.g_vars[i].name\n",
    "            curr_wvar  = [v for v in tf.global_variables() if v.name==curr_wname][0]\n",
    "            curr_wval  = _sess.run(curr_wvar)\n",
    "            \n",
    "            curr_wval_sqz = curr_wval\n",
    "            # curr_wval_sqz  = curr_wval.squeeze() # ???\n",
    "            curr_wval_sqz = np.asanyarray(curr_wval_sqz,order=(1,-1))\n",
    "            \n",
    "            self.g_wnames.append(curr_wname)\n",
    "            self.g_wvals.append(curr_wval_sqz)\n",
    "            self.g_wshapes.append(curr_wval.shape)\n",
    "        \"\"\" Save \"\"\"\n",
    "        np.savez(_savename,g_wnames=self.g_wnames,g_wvals=self.g_wvals,g_wshapes=self.g_wshapes)\n",
    "        if self.VERBOSE:\n",
    "            print (\"[%s] Saved. Size is [%.4f]MB\" % \n",
    "                   (_savename,os.path.getsize(_savename)/1000./1000.))\n",
    "        \n",
    "    # Restore\n",
    "    def restore(self,_sess,_loadname=None):\n",
    "        if _loadname==None:\n",
    "            _loadname='net/net_%s.npz'%(self.name)\n",
    "        l = np.load(_loadname)\n",
    "        g_wnames = l['g_wnames']\n",
    "        g_wvals  = l['g_wvals']\n",
    "        g_wshapes = l['g_wshapes']\n",
    "        for widx,wname in enumerate(g_wnames):\n",
    "            curr_wvar  = [v for v in tf.global_variables() if v.name==wname][0]\n",
    "            _sess.run(tf.assign(curr_wvar,g_wvals[widx].reshape(g_wshapes[widx])))\n",
    "        if self.VERBOSE:\n",
    "            print (\"Weight restored from [%s] Size is [%.4f]MB\" % \n",
    "                   (_loadname,os.path.getsize(_loadname)/1000./1000.))\n",
    "    \n",
    "    # Train \n",
    "    def train(self,_sess,_trainimg,_trainlabel,_testimg,_testlabel,_valimg,_vallabel,\n",
    "              _maxEpoch=10,_batchSize=256,_lr=1e-3,_kp=0.8,\n",
    "              _LR_SCHEDULE=False,_PRINT_EVERY=10,_SAVE_BEST=True,_DO_AUGMENTATION=False,_VERBOSE_TRAIN=True,\n",
    "              _seed=0):\n",
    "        tf.set_random_seed(_seed)\n",
    "        nTrain,nVal,nTest = _trainimg.shape[0],_valimg.shape[0],_testimg.shape[0]\n",
    "        txtName = ('res/res_%s.txt'%(self.name))\n",
    "        f = open(txtName,'w') # Open txt file\n",
    "        print_n_txt(_f=f,_chars='Text name: '+txtName)\n",
    "        print_period=max(1,_maxEpoch//_PRINT_EVERY)\n",
    "        print (\"print_period:[%d]\"%(print_period))\n",
    "        maxIter,maxValAccr,maxTestAccr = max(nTrain//_batchSize,1),0.0,0.0\n",
    "        for epoch in range(_maxEpoch+1): # For every epoch \n",
    "            self.epoch = epoch\n",
    "            _trainimg,_trainlabel = shuffle(_trainimg,_trainlabel) \n",
    "            for iter in range(maxIter): # For every iteration in one epoch\n",
    "                start,end = iter*_batchSize,(iter+1)*_batchSize\n",
    "                # Learning rate scheduling\n",
    "                if _LR_SCHEDULE:\n",
    "                    if epoch < 0.5*_maxEpoch:\n",
    "                        _lr_use = _lr\n",
    "                    elif epoch < 0.75*_maxEpoch:\n",
    "                        _lr_use = _lr/2.0\n",
    "                    else:\n",
    "                        _lr_use = _lr/10.0\n",
    "                else:\n",
    "                    _lr_use = _lr\n",
    "                if _DO_AUGMENTATION:\n",
    "                    trainImgBatch = augment_img(_trainimg[start:end,:],self.xdim) \n",
    "                else:\n",
    "                    trainImgBatch = _trainimg[start:end,:]\n",
    "                trainlabelBatch = _trainlabel[start:end,:]\n",
    "                if self.USE_MIXUP:\n",
    "                    trainImgBatch,trainlabelBatch = mixup(trainImgBatch,trainlabelBatch,self.mixup_val) # \n",
    "                feeds = {self.x:trainImgBatch,self.t:trainlabelBatch\n",
    "                         ,self.rho_ref:self.rho_ref_train,self.kp:_kp,self.lr:_lr_use,self.is_training:True}\n",
    "                _sess.run(self.optm,feed_dict=feeds) \n",
    "                \n",
    "            # Print training losses, training accuracy, validation accuracy, and test accuracy\n",
    "            if (epoch%print_period)==0 or (epoch==(_maxEpoch)):\n",
    "                batchSize4print = 512 \n",
    "                # Compute train loss and accuracy\n",
    "                maxIter4print = max(nTrain//batchSize4print,1)\n",
    "                trainLoss,trainAccr,fit,reg,kl,l2,nTemp = 0,0,0,0,0,0,0\n",
    "                for iter in range(maxIter4print):\n",
    "                    start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "                    feeds_train = {self.x:_trainimg[start:end,:],self.t:_trainlabel[start:end,:]\n",
    "                                   ,self.rho_ref:1.0,self.kp:1.0,self.is_training:False}\n",
    "                    opers_train = [self.loss_total,self.accr,self.loss_fit,self.loss_reg,self.kl_reg,self.l2_reg]\n",
    "                    _trainLoss,_trainAccr,_fit,_reg,_kl,_l2 = _sess.run(opers_train,feed_dict=feeds_train) \n",
    "                    _nTemp = end-start; nTemp+=_nTemp\n",
    "                    trainLoss+=(_nTemp*_trainLoss);trainAccr+=(_nTemp*_trainAccr)\n",
    "                    fit+=(_nTemp*_fit);reg+=(_nTemp*_reg);kl+=(_nTemp*_kl);l2+=(_nTemp*_l2)\n",
    "                trainLoss/=nTemp;trainAccr/=nTemp\n",
    "                fit/=nTemp;reg/=nTemp;kl/=nTemp;l2/=nTemp;\n",
    "                # Compute validation loss and accuracy\n",
    "                maxIter4print = max(nVal//batchSize4print,1)\n",
    "                valLoss,valAccr,nTemp = 0,0,0\n",
    "                for iter in range(maxIter4print):\n",
    "                    start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "                    feeds_val = {self.x:_valimg[start:end,:],self.t:_vallabel[start:end,:]\n",
    "                                 ,self.rho_ref:1.0,self.kp:1.0,self.is_training:False}\n",
    "                    _valLoss,_valAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_val) \n",
    "                    _nTemp = end-start; nTemp+=_nTemp\n",
    "                    valLoss+=(_nTemp*_valLoss); valAccr+=(_nTemp*_valAccr)\n",
    "                valLoss/=nTemp;valAccr/=nTemp \n",
    "                # Compute test loss and accuracy\n",
    "                maxIter4print = max(nTest//batchSize4print,1)\n",
    "                testLoss,testAccr,nTemp = 0,0,0\n",
    "                for iter in range(maxIter4print):\n",
    "                    start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "                    feeds_test = {self.x:_testimg[start:end,:],self.t:_testlabel[start:end,:]\n",
    "                                  ,self.rho_ref:1.0,self.kp:1.0,self.is_training:False}\n",
    "                    _testLoss,_testAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_test) \n",
    "                    _nTemp = end-start; nTemp+=_nTemp\n",
    "                    testLoss+=(_nTemp*_testLoss); testAccr+=(_nTemp*_testAccr)\n",
    "                testLoss/=nTemp;testAccr/=nTemp\n",
    "                # Compute max val accr \n",
    "                if valAccr > maxValAccr:\n",
    "                    maxValAccr = valAccr\n",
    "                    maxTestAccr = testAccr\n",
    "                    if _SAVE_BEST: self.save(_sess) \n",
    "                strTemp = ((\"[%02d/%d] [Loss] train:%.3f(f:%.3f+r:%.3f+k:%.3f+l:%.3f) val:%.3f test:%.3f\"\n",
    "                            +\" [Accr] train:%.1f%% val:%.1f%% test:%.1f%% maxVal:%.1f%% maxTest:%.1f%%\")\n",
    "                       %(epoch,_maxEpoch,trainLoss,fit,reg,kl,l2,valLoss,testLoss\n",
    "                         ,trainAccr*100,valAccr*100,testAccr*100,maxValAccr*100,maxTestAccr*100))\n",
    "                print_n_txt(_f=f,_chars=strTemp,_DO_PRINT=_VERBOSE_TRAIN)\n",
    "        # Done \n",
    "        print (\"Training finished.\")\n",
    "    \n",
    "    # Test\n",
    "    def test(self,_sess,_trainimg,_trainlabel,_testimg,_testlabel,_valimg,_vallabel):\n",
    "        nTrain,nVal,nTest = _trainimg.shape[0],_valimg.shape[0],_testimg.shape[0]\n",
    "        # Check accuracies (train, val, and test)\n",
    "        batchSize4print = 512 \n",
    "        # Compute train loss and accuracy\n",
    "        maxIter4print = max(nTrain//batchSize4print,1)\n",
    "        trainLoss,trainAccr,nTemp = 0,0,0\n",
    "        for iter in range(maxIter4print):\n",
    "            start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "            feeds_train = {self.x:_trainimg[start:end,:],self.t:_trainlabel[start:end,:]\n",
    "                           ,self.rho_ref:1.0,self.kp:1.0,self.is_training:False}\n",
    "            _trainLoss,_trainAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_train) \n",
    "            _nTemp = end-start; nTemp+=_nTemp\n",
    "            trainLoss+=(_nTemp*_trainLoss); trainAccr+=(_nTemp*_trainAccr)\n",
    "        trainLoss/=nTemp;trainAccr/=nTemp\n",
    "        # Compute validation loss and accuracy\n",
    "        maxIter4print = max(nVal//batchSize4print,1)\n",
    "        valLoss,valAccr,nTemp = 0,0,0\n",
    "        for iter in range(maxIter4print):\n",
    "            start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "            feeds_val = {self.x:_valimg[start:end,:],self.t:_vallabel[start:end,:]\n",
    "                         ,self.rho_ref:1.0,self.kp:1.0,self.is_training:False}\n",
    "            _valLoss,_valAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_val) \n",
    "            _nTemp = end-start; nTemp+=_nTemp\n",
    "            valLoss+=(_nTemp*_valLoss); valAccr+=(_nTemp*_valAccr)\n",
    "        valLoss/=nTemp;valAccr/=nTemp\n",
    "        # Compute test loss and accuracy\n",
    "        maxIter4print = max(nTest//batchSize4print,1)\n",
    "        testLoss,testAccr,nTemp = 0,0,0\n",
    "        for iter in range(maxIter4print):\n",
    "            start,end = iter*batchSize4print,(iter+1)*batchSize4print\n",
    "            feeds_test = {self.x:_testimg[start:end,:],self.t:_testlabel[start:end,:]\n",
    "                          ,self.rho_ref:1.0,self.kp:1.0,self.is_training:False}\n",
    "            _testLoss,_testAccr = _sess.run([self.loss_total,self.accr],feed_dict=feeds_test) \n",
    "            _nTemp = end-start; nTemp+=_nTemp\n",
    "            testLoss+=(_nTemp*_testLoss); testAccr+=(_nTemp*_testAccr)\n",
    "        testLoss/=nTemp;testAccr/=nTemp\n",
    "        strTemp = ((\"[%s] [Loss] train:%.3f val:%.3f test:%.3f\"\n",
    "                    +\" [Accr] train:%.3f%% val:%.3f%% test:%.3f%%\")\n",
    "               %(self.name,trainLoss,valLoss,testLoss,trainAccr*100,valAccr*100,testAccr*100))\n",
    "        print(strTemp)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    print (\"cn_cls_class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # noise_type = [pairflip, symmetric]\n",
    "    noise_type = 'pairflip' \n",
    "    noise_rate = 0.45\n",
    "    train_dataset = CIFAR10(root='./data/',download=True,train=True,transform=transforms.ToTensor(),\n",
    "                            noise_type=noise_type,noise_rate=noise_rate)\n",
    "    test_dataset = CIFAR10(root='./data/',download=True,train=False, transform=transforms.ToTensor(),\n",
    "                           noise_type=noise_type,noise_rate=noise_rate)\n",
    "    def one_hot(a, num_classes): # to 1-hot vectors\n",
    "        return np.squeeze(np.eye(num_classes)[a.reshape(-1)])\n",
    "    x_train = train_dataset.train_data/255. # rgb values between 0~1\n",
    "    n_train = np.shape(x_train)[0]\n",
    "    x_train = x_train.reshape([n_train,-1])\n",
    "    y_train = one_hot(train_dataset.train_labels,10)\n",
    "    y_noisy_train = one_hot(np.asarray(train_dataset.train_noisy_labels),10)\n",
    "    x_test = test_dataset.test_data/255. # rgb values between 0~1\n",
    "    n_test = np.shape(x_test)[0]\n",
    "    x_test = x_test.reshape([n_test,-1])\n",
    "    y_test = one_hot(np.asarray(test_dataset.test_labels),10)\n",
    "    label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "    # Print the shape of data\n",
    "    print (x_train.shape,y_train.shape,y_noisy_train.shape,)\n",
    "    print (x_test.shape,y_test.shape,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    CN = cn_cls_class(_name='cn_cls_cifar10',_xdim=[32,32,3],_ydim=10,\n",
    "                      _hdims=[128,128,128,256,256,256,512,256,128],_filterSizes=[3,3,3,3,3,3,3,3,3],\n",
    "                      _max_pools=[1,1,2,1,1,2,1,1,2],_feat_dim=128,\n",
    "                      _kmix=10,_actv=tf.nn.leaky_relu,_bn=slim.batch_norm,\n",
    "                      _rho_ref_train=0.95,_tau_inv=1e-4,_pi1_bias=0.0,_logSigmaZval=-2,\n",
    "                      _logsumexp_coef=1e-4,_kl_reg_coef=1e-4,_l2_reg_coef=1e-6,\n",
    "                      _momentum = 0.5,\n",
    "                      _USE_INPUT_BN=False,_USE_RESNET=False,_USE_GAP=False,_USE_KENDALL_LOSS=False,_USE_SGD=True,\n",
    "                      _GPU_ID=0,_VERBOSE=True)\n",
    "    sess = gpusession(); sess.run(tf.global_variables_initializer()) \n",
    "    CN.train(_sess=sess,\n",
    "             _trainimg=x_train,_trainlabel=y_noisy_train,\n",
    "             _testimg=x_test,_testlabel=y_test,\n",
    "             _valimg=x_test,_vallabel=y_test,\n",
    "             _maxEpoch=100,_batchSize=256,_lr=1e-2,_LR_SCHEDULE=True,_kp=0.9,_PRINT_EVERY=100,_SAVE_BEST=False,\n",
    "             _DO_AUGMENTATION=True,_VERBOSE_TRAIN=True)\n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
